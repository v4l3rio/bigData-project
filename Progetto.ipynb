{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani\n",
    "\n",
    "intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784973_Francesco_Magnani', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8c6743581d4e8f9fe5b7f013b65aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1707834106787_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-88.ec2.internal:20888/proxy/application_1707834106787_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-LQDU02XWM7MK\n",
       "\" application-id=\"application_1707834106787_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-29-167.ec2.internal:8042/node/containerlogs/container_1707834106787_0002_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-fmagnani\n",
      "path_book_sample: String = s3a://unibo-bd2324-fmagnani/project/book_sample134.json\n",
      "path_book_sample32row: String = s3a://unibo-bd2324-fmagnani/project/book_sample32row.json\n",
      "path_positive_words: String = s3a://unibo-bd2324-fmagnani/project/positive-words.txt\n",
      "path_negative_words: String = s3a://unibo-bd2324-fmagnani/project/negative-words.txt\n",
      "res3: String = SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/application_1707834106787_0002/\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-fmagnani\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_book_sample32row = \"s3a://\"+bucketname+\"/project/book_sample32row.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1760096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e379a046bca345d7b69c8a9399bb549e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.broadcast.Broadcast\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.broadcast.Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aa1bbae45144599445681b43ca5adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class BookReview\n",
      "defined object BookReview\n",
      "warning: previously defined class BookReview is not a companion to object BookReview.\n",
      "Companions must be defined together; you may wish to use :paste mode for this.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52289f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b40fd29bba4fcb9aae290bd1eb105c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookSampleRdd32row: org.apache.spark.sql.Dataset[BookReview] = [id: string, overall: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample).map(BookReview.extract)\n",
    "//val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample32row).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b272049e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fb1f179b7e4d0a9e8984888e3f14e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positiveWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-fmagnani/project/positive-words.txt MapPartitionsRDD[5] at textFile at <console>:27\n",
      "negativeWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-fmagnani/project/negative-words.txt MapPartitionsRDD[7] at textFile at <console>:27\n"
     ]
    }
   ],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea7ed53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc701ee21610426f98db5e051aa7705f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitAndClean: (id: String, text: String)Array[(String, String, Int)]\n"
     ]
    }
   ],
   "source": [
    "def splitAndClean(id:String, text: String): Array[(String, String, Int)] = \n",
    "    text.split(\"\\\\s+\").map(el => (id, el.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \"\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6868",
   "metadata": {},
   "source": [
    "# Tentativi di ottimizzazione\n",
    "1. utilizzo di variabili broadcast per le liste di parole uguali per tutti\n",
    "2. ripartizione dei dataset per ottimizzare il join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37560965",
   "metadata": {},
   "source": [
    "# Prima fase del Job 1\n",
    "\n",
    "Recensioni divise in parole tramite flat map, dopodiché join con parole positive e negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6505f5e32e8e44ae855d6a692e136f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countOccurrenciesOfWordReview: (reviews: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
      "joinAndMapWords: (review: org.apache.spark.sql.DataFrame, wordsDataset: org.apache.spark.broadcast.Broadcast[Array[String]])org.apache.spark.sql.DataFrame\n",
      "reviewsDataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, word: string ... 1 more field]\n",
      "positiveWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(99)\n",
      "negativeWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(101)\n",
      "withPositiveDF: org.apache.spark.sql.DataFrame = [id: string, occurrencies: bigint]\n",
      "withNegativeDF: org.apache.spark.sql.DataFrame = [id: string, occurrencies: bigint]\n",
      "reducedResult: org.apache.spark.sql.DataFrame = [id: string, valuePositive: bigint ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "def countOccurrenciesOfWordReview(reviews: DataFrame): DataFrame = \n",
    "    reviews.\n",
    "        groupBy(\"id\", \"word\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "\n",
    "\n",
    "def joinAndMapWords(review:DataFrame, wordsDataset: Broadcast[Array[String]]): DataFrame = {\n",
    "    val wordDF = review.sparkSession.createDataFrame(wordsDataset.value.map(Tuple1.apply)).toDF(\"word\")\n",
    "    \n",
    "    val aggregRevDF = countOccurrenciesOfWordReview(review)\n",
    "    \n",
    "    return aggregRevDF.\n",
    "        join(wordDF, aggregRevDF(\"word\") === wordDF(\"word\")).\n",
    "        groupBy(\"id\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "val reviewsDataset = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    filter(review => review.getAs[String](\"reviewText\") != null && review.getAs[String](\"reviewText\").length > 3).\n",
    "    flatMap(review => splitAndClean(review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    filter(row => row._2.length > 3).\n",
    "    toDF(\"id\", \"word\", \"occurrencies\").\n",
    "    cache()\n",
    "\n",
    "val positiveWordsBroadcast = sc.broadcast(positiveWords.collect())\n",
    "val negativeWordsBroadcast = sc.broadcast(negativeWords.collect())\n",
    "\n",
    "\n",
    "val withPositiveDF = joinAndMapWords(reviewsDataset, positiveWordsBroadcast)\n",
    "    \n",
    "val withNegativeDF = joinAndMapWords(reviewsDataset, negativeWordsBroadcast)\n",
    "\n",
    "\n",
    "val reducedResult = withPositiveDF.\n",
    "    join(withNegativeDF, Seq(\"id\"), \"outer\").\n",
    "    toDF(\"id\", \"valuePositive\", \"valueNegative\").\n",
    "    withColumn(\"estimated_overall\", coalesce(col(\"valuePositive\"), lit(0L)) - coalesce(col(\"valueNegative\"), lit(0L)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fb8aa",
   "metadata": {},
   "source": [
    "# Seconda fase del Job 1 - Calcolo risultati\n",
    "\n",
    "Qui calcoliamo i risultati utilizzando una normalizzazione Z-Score, successivamente normalizzata a sua volta tramite Min-Max tra 1 e 5, in modo da confrontarli con i voti delle recensioni stesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d78cec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad29c0a1ffa4952a80860afea51c972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultDataframe: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, estimated_overall: bigint]\n",
      "minValue: Double = -82.0\n",
      "maxValue: Double = 91.0\n",
      "meanAndStdDF: Array[org.apache.spark.sql.Row] = Array([1.8817995696529966,3.5709445103782658])\n",
      "meanValue: Double = 1.8817995696529966\n",
      "stdValue: Double = 3.5709445103782658\n",
      "normalizedDF: org.apache.spark.sql.DataFrame = [id: string, estimated_overall: bigint ... 1 more field]\n",
      "minMaxDF: Array[org.apache.spark.sql.Row] = Array([-23.490087657723784,24.956478649091867])\n",
      "minZScore: Double = -23.490087657723784\n",
      "maxZScore: Double = 24.956478649091867\n",
      "normalizedAndScaledDF: org.apache.spark.sql.DataFrame = [id: string, estimated_overall: double]\n",
      "mapIntToOverallVote: (toMap: Double, fromMin: Double, fromMax: Double, toMin: Double, toMax: Double)Double\n"
     ]
    }
   ],
   "source": [
    "val resultDataframe = reducedResult.select(\"id\", \"estimated_overall\").cache()\n",
    "\n",
    "val minValue = resultDataframe.agg(min(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "val maxValue = resultDataframe.agg(max(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "\n",
    "\n",
    "// Calculate mean and standard deviation\n",
    "\n",
    "val meanAndStdDF = resultDataframe.agg(mean(\"estimated_overall\"), stddev(\"estimated_overall\")).collect()\n",
    "val meanValue = meanAndStdDF(0)(0).asInstanceOf[Double]\n",
    "val stdValue = meanAndStdDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "// Normalize the data using Z-score normalization formula\n",
    "\n",
    "val normalizedDF = resultDataframe.withColumn(\"z_score\", (col(\"estimated_overall\") - meanValue) / stdValue)\n",
    "\n",
    "// Scale the normalized values to be between -1 and 1\n",
    "\n",
    "val minMaxDF = normalizedDF.agg(min(\"z_score\"), max(\"z_score\")).collect()\n",
    "val minZScore = minMaxDF(0)(0).asInstanceOf[Double]\n",
    "val maxZScore = minMaxDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "\n",
    "val normalizedAndScaledDF = normalizedDF.withColumn(\"estimated_overall\", \n",
    "                                                    (col(\"z_score\") - minZScore) / (maxZScore - minZScore) * 4 + 1).\n",
    "    select(\"id\", \"estimated_overall\")\n",
    "\n",
    "// DEPRECATED\n",
    "// a = min, b = max, c = 1 e d = 5\n",
    "def mapIntToOverallVote(toMap: Double, fromMin: Double, fromMax: Double, toMin:Double=1, toMax:Double=5): Double =\n",
    "     toMin + ((toMax-toMin)/(fromMax-fromMin))*(toMap-fromMin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42b92ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad3db794900426681310ea5ae078594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relativeErrors: org.apache.spark.sql.DataFrame = [id: string, real_value: double ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val relativeErrors = \n",
    "bookSampleRdd32row.\n",
    "    map(review => (review.id, review.overall)).\n",
    "    toDF(\"id\", \"real_value\").\n",
    "    join(normalizedAndScaledDF, Seq(\"id\")).\n",
    "    withColumn(\"relative_error\", abs(coalesce(col(\"real_value\"), lit(3.0)) - \n",
    "                                     coalesce(col(\"estimated_overall\"), lit(3.0))) / \n",
    "                                   coalesce(col(\"real_value\"), lit(3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2bec6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9aa049f804b918db55f693e041a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumOfRelativeErrors: Double = 74703.89026974584\n",
      "meanError: Double = 0.4240178581671454\n",
      "Errore Medio: 42.401785816714536%\n"
     ]
    }
   ],
   "source": [
    "val sumOfRelativeErrors = relativeErrors.select(\"relative_error\").agg(sum(\"relative_error\")).\n",
    "                                collect()(0).getDouble(0)\n",
    "val meanError =  sumOfRelativeErrors / relativeErrors.count\n",
    "\n",
    "println(\"Errore Medio: \" + meanError*100 + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b1fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a7823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
