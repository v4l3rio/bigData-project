{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani\n",
    "\n",
    "intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784973_Francesco_Magnani', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>application_1708009736490_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0005/\" class=\"emr-proxy-link\" emr-resource=\"j-GAOU7ABEB65D\n",
       "\" application-id=\"application_1708009736490_0005\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-30-112.ec2.internal:8042/node/containerlogs/container_1708009736490_0005_01_000001/livy\" >Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d945b87eb35449f3b3c1bb880c41ab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-fmagnani\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_book_sample32row = \"s3a://\"+bucketname+\"/project/book_sample32row.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1760096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892b81d1b08c4cd9b466faa50198271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.broadcast.Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e28faedc8d34b25bbb9c95c62fd7a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78959e79",
   "metadata": {},
   "source": [
    "## Caricamento del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52289f8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9273f027b94a1f8c0e452455c74558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "//val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample).map(BookReview.extract)\n",
    "val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample32row).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd791fd",
   "metadata": {},
   "source": [
    "# Job 1 - Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272049e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781d895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7ed53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def splitAndClean(id:String, text: String): Array[(String, String, Int)] = \n",
    "    text.split(\"\\\\s+\").map(el => (id, el.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \"\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6868",
   "metadata": {},
   "source": [
    "# Tentativi di ottimizzazione\n",
    "1. utilizzo di variabili broadcast per le liste di parole uguali per tutti\n",
    "2. ripartizione dei dataset per ottimizzare il join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37560965",
   "metadata": {},
   "source": [
    "# Prima fase del Job 1\n",
    "\n",
    "Recensioni divise in parole tramite flat map, dopodichÃ© join con parole positive e negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def countOccurrenciesOfWordReview(reviews: DataFrame): DataFrame = \n",
    "    reviews.\n",
    "        groupBy(\"id\", \"word\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "\n",
    "\n",
    "def joinAndMapWords(review:DataFrame, wordsDataset: Broadcast[Array[String]]): DataFrame = {\n",
    "    val wordDF = review.sparkSession.createDataFrame(wordsDataset.value.map(Tuple1.apply)).toDF(\"word\")\n",
    "    \n",
    "    val aggregRevDF = countOccurrenciesOfWordReview(review)\n",
    "    \n",
    "    return aggregRevDF.\n",
    "        join(wordDF, aggregRevDF(\"word\") === wordDF(\"word\")).\n",
    "        groupBy(\"id\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "val reviewsDataset = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    filter(review => review.getAs[String](\"reviewText\") != null && review.getAs[String](\"reviewText\").length > 3).\n",
    "    flatMap(review => splitAndClean(review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    filter(row => row._2.length > 3).\n",
    "    toDF(\"id\", \"word\", \"occurrencies\").\n",
    "    cache()\n",
    "\n",
    "val positiveWordsBroadcast = sc.broadcast(positiveWords.collect())\n",
    "val negativeWordsBroadcast = sc.broadcast(negativeWords.collect())\n",
    "\n",
    "\n",
    "val withPositiveDF = joinAndMapWords(reviewsDataset, positiveWordsBroadcast)\n",
    "    \n",
    "val withNegativeDF = joinAndMapWords(reviewsDataset, negativeWordsBroadcast)\n",
    "\n",
    "\n",
    "val reducedResult = withPositiveDF.\n",
    "    join(withNegativeDF, Seq(\"id\"), \"outer\").\n",
    "    toDF(\"id\", \"valuePositive\", \"valueNegative\").\n",
    "    withColumn(\"estimated_overall\", coalesce(col(\"valuePositive\"), lit(0L)) - coalesce(col(\"valueNegative\"), lit(0L)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fb8aa",
   "metadata": {},
   "source": [
    "# Seconda fase del Job 1 - Calcolo risultati\n",
    "\n",
    "Qui calcoliamo i risultati utilizzando una normalizzazione Z-Score, successivamente normalizzata a sua volta tramite Min-Max tra 1 e 5, in modo da confrontarli con i voti delle recensioni stesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78cec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val resultDataframe = reducedResult.select(\"id\", \"estimated_overall\").cache()\n",
    "\n",
    "val minValue = resultDataframe.agg(min(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "val maxValue = resultDataframe.agg(max(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "\n",
    "\n",
    "// Calculate mean and standard deviation\n",
    "\n",
    "val meanAndStdDF = resultDataframe.agg(mean(\"estimated_overall\"), stddev(\"estimated_overall\")).collect()\n",
    "val meanValue = meanAndStdDF(0)(0).asInstanceOf[Double]\n",
    "val stdValue = meanAndStdDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "// Normalize the data using Z-score normalization formula\n",
    "\n",
    "val normalizedDF = resultDataframe.withColumn(\"z_score\", (col(\"estimated_overall\") - meanValue) / stdValue)\n",
    "\n",
    "// Scale the normalized values to be between -1 and 1\n",
    "\n",
    "val minMaxDF = normalizedDF.agg(min(\"z_score\"), max(\"z_score\")).collect()\n",
    "val minZScore = minMaxDF(0)(0).asInstanceOf[Double]\n",
    "val maxZScore = minMaxDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "\n",
    "val normalizedAndScaledDF = normalizedDF.withColumn(\"estimated_overall\", \n",
    "                                                    (col(\"z_score\") - minZScore) / (maxZScore - minZScore) * 4 + 1).\n",
    "    select(\"id\", \"estimated_overall\")\n",
    "\n",
    "// DEPRECATED\n",
    "// a = min, b = max, c = 1 e d = 5\n",
    "def mapIntToOverallVote(toMap: Double, fromMin: Double, fromMax: Double, toMin:Double=1, toMax:Double=5): Double =\n",
    "     toMin + ((toMax-toMin)/(fromMax-fromMin))*(toMap-fromMin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b92ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val relativeErrors = \n",
    "bookSampleRdd32row.\n",
    "    map(review => (review.id, review.overall)).\n",
    "    toDF(\"id\", \"real_value\").\n",
    "    join(normalizedAndScaledDF, Seq(\"id\")).\n",
    "    withColumn(\"relative_error\", abs(coalesce(col(\"real_value\"), lit(3.0)) - \n",
    "                                     coalesce(col(\"estimated_overall\"), lit(3.0))) / \n",
    "                                   coalesce(col(\"real_value\"), lit(3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sumOfRelativeErrors = relativeErrors.select(\"relative_error\").agg(sum(\"relative_error\")).\n",
    "                                collect()(0).getDouble(0)\n",
    "val meanError =  sumOfRelativeErrors / relativeErrors.count\n",
    "\n",
    "println(\"Errore Medio: \" + meanError*100 + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ea502d1",
   "metadata": {},
   "source": [
    "# Job 2 - Frequent Pattern Mining\n",
    "\n",
    "Qui inizia il secondo Job, dove effettuiamo un Frequent Pattern Mining utilizzando le funzionalitÃ  di Spark.\n",
    "\n",
    "## Creazione del DataFrame delle reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682a8692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1b6f5583414ed9bc5e918c212fd3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "def clean(text: String): String =\n",
    "    text.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ad9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caddc75090c4fafafaf6226fe4cf911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "val reviewsDF = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    map(review => (review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    rdd.\n",
    "    filter({case (_, text) => text != null && text.length > 3}).\n",
    "    map({case (id, text) => (id, clean(text))}).\n",
    "    toDF(\"id\", \"text\").\n",
    "    cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b674de6",
   "metadata": {},
   "source": [
    "## Creazione di Tokenizer e StopWord remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73a7823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b734b983685445179887b96a79a970ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "\n",
    "val tokenizer = new Tokenizer().\n",
    "    setInputCol(\"text\").\n",
    "    setOutputCol(\"words\")\n",
    "\n",
    "val remover = new StopWordsRemover().\n",
    "    setInputCol(\"words\").\n",
    "    setOutputCol(\"filtered_words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a04f3",
   "metadata": {},
   "source": [
    "## Pulizia del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5cc010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c4c5a51037474f9fb4f1993a25ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    "\n",
    "val wordsDF = tokenizer.transform(reviewsDF)\n",
    "val filteredWordsDF = remover.transform(wordsDF)\n",
    "\n",
    "val uniqueWordsDF = filteredWordsDF.\n",
    "    select(\"filtered_words\").\n",
    "    distinct().\n",
    "    map(item => item.getAs[WrappedArray[String]](0).filter(_.length > 1).toArray.distinct).\n",
    "    toDF(\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956c265",
   "metadata": {},
   "source": [
    "## Utilizzo di FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1553cd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ab8adeabd346f1bf856f7741b361d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "\n",
    "def frequentItemsets(df: DataFrame): DataFrame = {\n",
    "    val fpg = new FPGrowth().\n",
    "        setItemsCol(\"words\").\n",
    "        setMinSupport(0.1).\n",
    "        setMinConfidence(0.5)\n",
    "\n",
    "    val model = fpg.fit(df)\n",
    "    return model.freqItemsets\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44fbd",
   "metadata": {},
   "source": [
    "## Visualizzazione dei risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentItemsets(uniqueWordsDF).orderBy($\"freq\".desc).show(50)\n",
    "\n",
    "// transform examines the input items against all the association rules and summarize the\n",
    "// consequents as prediction\n",
    "model.transform(uniqueWordsDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dfd38",
   "metadata": {},
   "source": [
    "## Secondo Dataset, rimuovendo parole triviali (es. \"book\", \"read\" etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51f6a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d861a9ce524bc0bf3604f9335d28c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "// caricamento della blacklist delle parole da escludere\n",
    "val path_blacklist_words = \"s3a://\"+bucketname+\"/project/blacklist.txt\"\n",
    "\n",
    "val blacklistWordsDF = sc.textFile(path_blacklist_words).toDF(\"words\")\n",
    "\n",
    "// Explode the array of strings in dfArray\n",
    "val explodedDF = uniqueWordsDF.withColumn(\"word\", explode(col(\"words\")))\n",
    "\n",
    "// Perform subtraction\n",
    "val subtractedDF = explodedDF.\n",
    "    join(blacklistWordsDF, explodedDF(\"word\") === blacklistWordsDF(\"words\"), \"left_anti\").\n",
    "    select(\"word\").\n",
    "    distinct()\n",
    "\n",
    "// Group back into array\n",
    "val prunedDF = subtractedDF.\n",
    "    groupBy().\n",
    "    agg(collect_list(\"word\") as \"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997ce7e",
   "metadata": {},
   "source": [
    "## Secondo utilizzo di FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d666b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061b8171dcba445a912233cfa6d150d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 5 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Feb 15, 2024 5:29:49 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "24/02/15 17:29:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/15 17:29:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-23-54.ec2.internal/172.31.23.54:8032\n",
      "24/02/15 17:29:52 INFO Configuration: resource-types.xml not found\n",
      "24/02/15 17:29:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/02/15 17:29:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "24/02/15 17:29:52 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "24/02/15 17:29:52 INFO Client: Setting up container launch context for our AM\n",
      "24/02/15 17:29:52 INFO Client: Setting up the launch environment for our AM container\n",
      "24/02/15 17:29:52 INFO Client: Preparing resources for our AM container\n",
      "24/02/15 17:29:52 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/15 17:29:54 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_libs__3721909478527085093.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_libs__3721909478527085093.zip\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/kryo-shaded-4.0.2.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-api-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-rsc-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "24/02/15 17:29:55 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/minlog-1.3.0.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/netty-all-4.1.17.Final.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/objenesis-2.5.1.jar\n",
      "24/02/15 17:29:56 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/commons-codec-1.9.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-core_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/livy-repl_2.12-0.7.1-incubating.jar\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hive-site.xml\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/hudi-defaults.conf\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/sparkr.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/pyspark.zip\n",
      "24/02/15 17:29:57 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/py4j-0.10.9.7-src.zip\n",
      "24/02/15 17:29:58 INFO Client: Uploading resource file:/mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a/__spark_conf__11450964975075926862.zip -> hdfs://ip-172-31-23-54.ec2.internal:8020/user/livy/.sparkStaging/application_1708009736490_0006/__spark_conf__.zip\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls to: livy\n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/15 17:29:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "24/02/15 17:29:58 INFO Client: Submitting application application_1708009736490_0006 to ResourceManager\n",
      "24/02/15 17:29:58 INFO YarnClientImpl: Submitted application application_1708009736490_0006\n",
      "24/02/15 17:29:58 INFO Client: Application report for application_1708009736490_0006 (state: ACCEPTED)\n",
      "24/02/15 17:29:58 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1708018198675\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-23-54.ec2.internal:20888/proxy/application_1708009736490_0006/\n",
      "\t user: livy\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-62d8dbb4-76f1-4a37-87d1-ba5399fc079e\n",
      "24/02/15 17:29:58 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e1794332-473d-481e-89e3-f653d277316a\n",
      "\n",
      "YARN Diagnostics: \n",
      "[Thu Feb 15 17:29:58 +0000 2024] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:12288, vCores:4> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 65.625 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:12288, vCores:4> ; Queue's used capacity (absolute resource) = <memory:8064, vCores:2> ; Queue's max capacity (absolute resource) = <memory:12288, vCores:4> ; .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "frequentItemsets(prunedDF).orderBy($\"freq\".desc).show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
