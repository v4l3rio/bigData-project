{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani\n",
    "\n",
    "intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784973_Francesco_Magnani', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bfb416acb14040be457d04d63c0e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1708096619670_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-18-87.ec2.internal:20888/proxy/application_1708096619670_0003/\" class=\"emr-proxy-link\" emr-resource=\"j-28UZP3K326PZC\n",
       "\" application-id=\"application_1708096619670_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-21-194.ec2.internal:8042/node/containerlogs/container_1708096619670_0003_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-fmagnani\n",
      "path_book_sample: String = s3a://unibo-bd2324-fmagnani/project/book_sample134.json\n",
      "path_book_sample32row: String = s3a://unibo-bd2324-fmagnani/project/book_sample32row.json\n",
      "path_positive_words: String = s3a://unibo-bd2324-fmagnani/project/positive-words.txt\n",
      "path_negative_words: String = s3a://unibo-bd2324-fmagnani/project/negative-words.txt\n",
      "res3: String = SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/application_1708096619670_0003/\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-fmagnani\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_book_sample32row = \"s3a://\"+bucketname+\"/project/book_sample32row.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1760096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165fbbbd4764dfb90bc70f444c7b29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.broadcast.Broadcast\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.broadcast.Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ebe82068da4180bff0748d18cab48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class BookReview\n",
      "defined object BookReview\n",
      "warning: previously defined class BookReview is not a companion to object BookReview.\n",
      "Companions must be defined together; you may wish to use :paste mode for this.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78959e79",
   "metadata": {},
   "source": [
    "## Caricamento del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52289f8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7737d7134f4425eb0bf25d6cb30935e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookSampleRdd32row: org.apache.spark.sql.Dataset[BookReview] = [id: string, overall: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample).map(BookReview.extract)\n",
    "//val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample32row).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd791fd",
   "metadata": {},
   "source": [
    "# Job 1 - Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272049e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781d895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7ed53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def splitAndClean(id:String, text: String): Array[(String, String, Int)] = \n",
    "    text.split(\"\\\\s+\").map(el => (id, el.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \"\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6868",
   "metadata": {},
   "source": [
    "# Tentativi di ottimizzazione\n",
    "1. utilizzo di variabili broadcast per le liste di parole uguali per tutti\n",
    "2. ripartizione dei dataset per ottimizzare il join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37560965",
   "metadata": {},
   "source": [
    "# Prima fase del Job 1\n",
    "\n",
    "Recensioni divise in parole tramite flat map, dopodiché join con parole positive e negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def countOccurrenciesOfWordReview(reviews: DataFrame): DataFrame = \n",
    "    reviews.\n",
    "        groupBy(\"id\", \"word\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "\n",
    "\n",
    "def joinAndMapWords(review:DataFrame, wordsDataset: Broadcast[Array[String]]): DataFrame = {\n",
    "    val wordDF = review.sparkSession.createDataFrame(wordsDataset.value.map(Tuple1.apply)).toDF(\"word\")\n",
    "    \n",
    "    val aggregRevDF = countOccurrenciesOfWordReview(review)\n",
    "    \n",
    "    return aggregRevDF.\n",
    "        join(wordDF, aggregRevDF(\"word\") === wordDF(\"word\")).\n",
    "        groupBy(\"id\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "val reviewsDataset = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    filter(review => review.getAs[String](\"reviewText\") != null && review.getAs[String](\"reviewText\").length > 3).\n",
    "    flatMap(review => splitAndClean(review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    filter(row => row._2.length > 3).\n",
    "    toDF(\"id\", \"word\", \"occurrencies\").\n",
    "    cache()\n",
    "\n",
    "val positiveWordsBroadcast = sc.broadcast(positiveWords.collect())\n",
    "val negativeWordsBroadcast = sc.broadcast(negativeWords.collect())\n",
    "\n",
    "\n",
    "val withPositiveDF = joinAndMapWords(reviewsDataset, positiveWordsBroadcast)\n",
    "    \n",
    "val withNegativeDF = joinAndMapWords(reviewsDataset, negativeWordsBroadcast)\n",
    "\n",
    "\n",
    "val reducedResult = withPositiveDF.\n",
    "    join(withNegativeDF, Seq(\"id\"), \"outer\").\n",
    "    toDF(\"id\", \"valuePositive\", \"valueNegative\").\n",
    "    withColumn(\"estimated_overall\", coalesce(col(\"valuePositive\"), lit(0L)) - coalesce(col(\"valueNegative\"), lit(0L)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fb8aa",
   "metadata": {},
   "source": [
    "# Seconda fase del Job 1 - Calcolo risultati\n",
    "\n",
    "Qui calcoliamo i risultati utilizzando una normalizzazione Z-Score, successivamente normalizzata a sua volta tramite Min-Max tra 1 e 5, in modo da confrontarli con i voti delle recensioni stesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78cec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val resultDataframe = reducedResult.select(\"id\", \"estimated_overall\").cache()\n",
    "\n",
    "val minValue = resultDataframe.agg(min(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "val maxValue = resultDataframe.agg(max(\"estimated_overall\")).head().getLong(0).toDouble\n",
    "\n",
    "\n",
    "// Calculate mean and standard deviation\n",
    "\n",
    "val meanAndStdDF = resultDataframe.agg(mean(\"estimated_overall\"), stddev(\"estimated_overall\")).collect()\n",
    "val meanValue = meanAndStdDF(0)(0).asInstanceOf[Double]\n",
    "val stdValue = meanAndStdDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "// Normalize the data using Z-score normalization formula\n",
    "\n",
    "val normalizedDF = resultDataframe.withColumn(\"z_score\", (col(\"estimated_overall\") - meanValue) / stdValue)\n",
    "\n",
    "// Scale the normalized values to be between -1 and 1\n",
    "\n",
    "val minMaxDF = normalizedDF.agg(min(\"z_score\"), max(\"z_score\")).collect()\n",
    "val minZScore = minMaxDF(0)(0).asInstanceOf[Double]\n",
    "val maxZScore = minMaxDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "\n",
    "val normalizedAndScaledDF = normalizedDF.withColumn(\"estimated_overall\", \n",
    "                                                    (col(\"z_score\") - minZScore) / (maxZScore - minZScore) * 4 + 1).\n",
    "    select(\"id\", \"estimated_overall\")\n",
    "\n",
    "// DEPRECATED\n",
    "// a = min, b = max, c = 1 e d = 5\n",
    "def mapIntToOverallVote(toMap: Double, fromMin: Double, fromMax: Double, toMin:Double=1, toMax:Double=5): Double =\n",
    "     toMin + ((toMax-toMin)/(fromMax-fromMin))*(toMap-fromMin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b92ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val relativeErrors = \n",
    "bookSampleRdd32row.\n",
    "    map(review => (review.id, review.overall)).\n",
    "    toDF(\"id\", \"real_value\").\n",
    "    join(normalizedAndScaledDF, Seq(\"id\")).\n",
    "    withColumn(\"relative_error\", abs(coalesce(col(\"real_value\"), lit(3.0)) - \n",
    "                                     coalesce(col(\"estimated_overall\"), lit(3.0))) / \n",
    "                                   coalesce(col(\"real_value\"), lit(3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sumOfRelativeErrors = relativeErrors.select(\"relative_error\").agg(sum(\"relative_error\")).\n",
    "                                collect()(0).getDouble(0)\n",
    "val meanError =  sumOfRelativeErrors / relativeErrors.count\n",
    "\n",
    "println(\"Errore Medio: \" + meanError*100 + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ea502d1",
   "metadata": {},
   "source": [
    "# Job 2 - Frequent Pattern Mining\n",
    "\n",
    "Qui inizia il secondo Job, dove effettuiamo un Frequent Pattern Mining utilizzando le funzionalità di Spark.\n",
    "\n",
    "## Creazione del DataFrame delle reviews\n",
    "Prima di tutto andiamo a definire una funzione di utility che ci permette di eliminare eventuali caratteri estranei dai testi delle recensioni (es. virgole, punti etc.). \n",
    "\n",
    "Dopodiché, utilizziamo il Dataset caricato anche per il primo Job e ne prendiamo solo le colonne `\"id\"` e `\"reviewText\"`, sono le uniche che ci servono. In modo simile al primo Job, eliminiamo le recensioni duplicate usando ```dropDuplicates```: questo è necessario perché esplorando il Dataset sono state trovate recensioni identiche per lo stesso prodotto, facenti riferimento a diverse versioni dello stesso.\n",
    "\n",
    "Per comodità, il Dataset viene convertito in un RDD in modo da svolgere le successive operazioni di **filter** e **map**. Infine, otteniamo il risultato in forma di DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682a8692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed0d8e0bae44fc6b64a7b9e6abb0f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean: (text: String)String\n"
     ]
    }
   ],
   "source": [
    "def clean(text: String): String =\n",
    "    text.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09ad9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7677753897c847fca1b2a3a8973321c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, text: string]\n"
     ]
    }
   ],
   "source": [
    "val reviewsDF = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    map(review => (review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    rdd.\n",
    "    filter({case (_, text) => text != null && text.length > 3}).\n",
    "    map({case (id, text) => (id, clean(text))}).\n",
    "    toDF(\"id\", \"text\").\n",
    "    cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b674de6",
   "metadata": {},
   "source": [
    "## Creazione di Tokenizer e StopWord remover\n",
    "\n",
    "Queste due funzioni sono utili per **dividere i testi in parole** e **rimuovere le stop words** (parole comunemente utilizzate in un linguaggio ma normalmente prive di valore se prese da sole, per esempio \"is\", \"the\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73a7823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7127372ae6404c6d98b136c1501f7b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
      "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_3ec197d3b704\n",
      "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_c2335289e0fa, numStopWords=181, locale=en_US, caseSensitive=false\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "\n",
    "val tokenizer = new Tokenizer().\n",
    "    setInputCol(\"text\").\n",
    "    setOutputCol(\"words\")\n",
    "\n",
    "val remover = new StopWordsRemover().\n",
    "    setInputCol(\"words\").\n",
    "    setOutputCol(\"filtered_words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a04f3",
   "metadata": {},
   "source": [
    "## Pulizia del Dataset\n",
    "\n",
    "Nella cella seguente andiamo a utilizzare il `tokenizer` e il `remover` creati precedentemente per pulire ulteriormente il DataFrame. \n",
    "\n",
    "Osservando i risultati di `filteredWordsDF` si può notare che sono ancora presenti refusi e stringhe contenenti spazi. Oltre a questo, la classe che verrà utilizzata successivamente, ovvero `FPGrowth`, richiede che all'interno di ogni pattern siano presenti stringhe **univoche**, motivo per cui andiamo a svolgere l'operazione `toArray.distinct`, che rimuove elementi duplici all'interno di un'Array di Scala)\n",
    "\n",
    "Otteniamo, infine, un DataFrame da una sola colonna, contenente un Array di parole per ciascuna recensione. È possibile notare che non sono più presenti gli ID: questi infatti non sono più necessari al fine di computare i Frequent Pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c5cc010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b1f8646e0d41df8d0f14255cb8a5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.collection.mutable.WrappedArray\n",
      "wordsDF: org.apache.spark.sql.DataFrame = [id: string, text: string ... 1 more field]\n",
      "filteredWordsDF: org.apache.spark.sql.DataFrame = [id: string, text: string ... 2 more fields]\n",
      "uniqueWordsDF: org.apache.spark.sql.DataFrame = [words: array<string>]\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    "\n",
    "val wordsDF = tokenizer.transform(reviewsDF)\n",
    "val filteredWordsDF = remover.transform(wordsDF)\n",
    "\n",
    "val uniqueWordsDF = filteredWordsDF.\n",
    "    select(\"filtered_words\").\n",
    "    distinct().\n",
    "    map(item => item.getAs[WrappedArray[String]](0).filter(_.length > 1).toArray.distinct).\n",
    "    toDF(\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956c265",
   "metadata": {},
   "source": [
    "## Utilizzo di FPGrowth\n",
    "\n",
    "Nella cella seguente definiamo una funzione che ci permette di utilizzare facilmente la classe di `FPGrowth` per calcolare i Frequent Pattern di un DataFrame. Questa istanzia la classe in questione, fissando gli iper-parametri a dei valori arbitrari: in particolare, il **supporto minimo** per considerare un pattern *frequent* e la **confidenza minima**, ovvero (dalla documentazione di Spark) un'indicazione di quanto spesso una *association rule* viene trovata vera. Quest'ultima non influenza la ricerca dei Frequent Pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1553cd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda26be9544f48aeb7002401cfba55d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.fpm.FPGrowth\n",
      "frequentItemsets: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "\n",
    "def frequentItemsets(df: DataFrame): DataFrame = {\n",
    "    val fpg = new FPGrowth().\n",
    "        setItemsCol(\"words\").\n",
    "        setMinSupport(0.1).\n",
    "        setMinConfidence(0.5)\n",
    "\n",
    "    val model = fpg.fit(df)\n",
    "    return model.freqItemsets\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44fbd",
   "metadata": {},
   "source": [
    "## Visualizzazione dei risultati\n",
    "\n",
    "Qui effettuiamo una prima visualizzazione dei risultati, mostrandone 50. È possibile notare fin da subito come i risultati più frequenti siano parole come \"libro\" e \"leggere\", risultati piuttosto predicibili. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentItemsets(uniqueWordsDF).show(50)\n",
    "\n",
    "model.transform(uniqueWordsDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dfd38",
   "metadata": {},
   "source": [
    "## Secondo Dataset, rimuovendo parole triviali (es. \"book\", \"read\" etc.)\n",
    "\n",
    "Per eliminare queste parole dalla ricerca, effettuiamo un'ultima modifica al nostro DataFrame, eliminando parole triviali come \"libro\" e \"leggere\", prese da un file opportunamente creato per lo scopo. Questo viene poi tramutato in un array contenuto in una **broadcast variable**. Infine, ogni array di parole viene modificato in modo da sottrarre queste parole alla ricerca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b51f6a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7381d2e4c2144198976af6441f8027a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_blacklist_words: String = s3a://unibo-bd2324-fmagnani/project/blacklist.txt\n",
      "blacklistWords: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(122)\n",
      "prunedDF: org.apache.spark.sql.DataFrame = [words: array<string>]\n"
     ]
    }
   ],
   "source": [
    "// caricamento della blacklist delle parole da escludere\n",
    "val path_blacklist_words = \"s3a://\"+bucketname+\"/project/blacklist.txt\"\n",
    "\n",
    "val blacklistWords = sc.broadcast(sc.textFile(path_blacklist_words).collect)\n",
    "\n",
    "val prunedDF = uniqueWordsDF.\n",
    "    map(item => item.getAs[WrappedArray[String]](0).toArray.filterNot(blacklistWords.value.contains)).\n",
    "    toDF(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997ce7e",
   "metadata": {},
   "source": [
    "## Secondo utilizzo di FPGrowth\n",
    "\n",
    "Il risultato finale appare più interessante. Questo potrebbe essere ulteriormente migliorato escludendo più parole dalla ricerca.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d666b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b670b0c42c3b4548bfabc4265e006d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|       items| freq|\n",
      "+------------+-----+\n",
      "|     [story]|34175|\n",
      "|     [great]|32786|\n",
      "|      [good]|29632|\n",
      "|      [like]|27869|\n",
      "|      [time]|25746|\n",
      "|[characters]|23769|\n",
      "|      [love]|22455|\n",
      "|      [well]|22368|\n",
      "|     [first]|22238|\n",
      "|      [much]|21531|\n",
      "|    [series]|20257|\n",
      "|      [many]|18528|\n",
      "|    [really]|18458|\n",
      "|       [get]|17267|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frequentItemsets(prunedDF).show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
