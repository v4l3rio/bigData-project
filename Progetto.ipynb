{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani\n",
    "\n",
    "intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1707323170996_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-83-144.ec2.internal:20888/proxy/application_1707323170996_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-910LEEKWOO9R\n",
       "\" application-id=\"application_1707323170996_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-94-172.ec2.internal:8042/node/containerlogs/container_1707323170996_0004_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorMemory': '8G', 'numExecutors': 2, 'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784984_Valerio_Di_Zio', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1707323170996_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-83-144.ec2.internal:20888/proxy/application_1707323170996_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-910LEEKWOO9R\n",
       "\" application-id=\"application_1707323170996_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-94-172.ec2.internal:8042/node/containerlogs/container_1707323170996_0004_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2f46297e884f8e86a68eef66642d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-vdizio\n",
      "path_book_sample: String = s3a://unibo-bd2324-vdizio/project/book_sample134.json\n",
      "path_positive_words: String = s3a://unibo-bd2324-vdizio/project/positive-words.txt\n",
      "path_negative_words: String = s3a://unibo-bd2324-vdizio/project/negative-words.txt\n",
      "res3: String = SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/application_1707323170996_0004/\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-vdizio\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5294f757f17a4859bd194e8d76157dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class BookReview\n",
      "defined object BookReview\n",
      "warning: previously defined class BookReview is not a companion to object BookReview.\n",
      "Companions must be defined together; you may wish to use :paste mode for this.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52289f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d9533ad673421c925103297a335d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookSampleRdd: org.apache.spark.sql.Dataset[BookReview] = [id: string, overall: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "val bookSampleRdd = spark.read.json(path_book_sample).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "90af0f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daaaa74dd6142929a104e6c48b2662a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res33: Array[BookReview] =\n",
      "Array(BookReview(A2M08SO0PJKPAV-0001712799,1.0,02 20, 2016,A2M08SO0PJKPAV,0001712799,Completly boring!!! Yes it's a childerns book that they will be able to read beacuse 60% of the book is the word Up.\n",
      "This one never gets picked for story time just sits on the shelf.\n",
      "We love Dr Seuss books but this one is disappointing.,Don't waste your money), BookReview(A15579Y1NDKSGW-0002005263,1.0,07 14, 2007,A15579Y1NDKSGW,0002005263,Characters you don't care about, exposition instead of action, Leaphorn and Chee MIA for much of the story...this one has it all.  I gave it a generous chance for five chapters, then called it quits because it was uninteresting and uninvolving.\n",
      "\n",
      "Rob Schmidt\n",
      "BlueCornComics.com,Worst Hillerman mystery ever?), BookReview(A1JYERR10WFM3O-000200526...\n"
     ]
    }
   ],
   "source": [
    "bookSampleRdd.filter(_.overall<2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b272049e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4ebced09f74392b05f15009a03dcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positiveWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/positive-words.txt MapPartitionsRDD[62] at textFile at <console>:24\n",
      "negativeWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/negative-words.txt MapPartitionsRDD[64] at textFile at <console>:24\n"
     ]
    }
   ],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b585eed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0589732df42c4227883d607ea48c9c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countOccurrences: (src: String, tgt: String)Int\n",
      "countPositive: (text: String)Double\n",
      "countNegative: (text: String)Double\n"
     ]
    }
   ],
   "source": [
    "def countOccurrences(src: String, tgt: String): Int =\n",
    "  src.sliding(tgt.length).count(window => window == tgt)\n",
    "\n",
    "def countPositive(text: String) = positiveWords.map(word => countOccurrences(text, word)).sum\n",
    "def countNegative(text: String) = negativeWords.map(word => countOccurrences(text, word)).sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e9f631bf994f22b8d2436d0fd7d708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 19.0 failed 4 times, most recent failure: Lost task 4.3 in stage 19.0 (TID 70) (ip-172-31-83-211.ec2.internal executor 1): org.apache.spark.SparkException: This RDD lacks a SparkContext. It could happen in the following cases:\n",
      "(1) RDD transformations and actions are NOT invoked by the driver, but inside of other transformations; for example, rdd1.map(x => rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n",
      "(2) When a Spark Streaming job recovers from checkpoint, this exception will be hit if a reference to an RDD not defined by the streaming job is used in DStream operations. For more information, See SPARK-13758.\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.rddLacksSparkContextError(SparkCoreErrors.scala:130)\n",
      "\tat org.apache.spark.rdd.RDD.sc(RDD.scala:95)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.map(RDD.scala:412)\n",
      "\tat countPositive(<console>:25)\n",
      "\tat $anonfun$res38$3(<console>:29)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_2$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1263)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1263)\n",
      "  at scala.Option.foreach(Option.scala:407)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1263)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3173)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2288)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2307)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2332)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:465)\n",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4244)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3462)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4234)\n",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:570)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4232)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n",
      "  at org.apache.spark.sql.Dataset.collect(Dataset.scala:3462)\n",
      "  ... 51 elided\n",
      "Caused by: org.apache.spark.SparkException: This RDD lacks a SparkContext. It could happen in the following cases:\n",
      "(1) RDD transformations and actions are NOT invoked by the driver, but inside of other transformations; for example, rdd1.map(x => rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n",
      "(2) When a Spark Streaming job recovers from checkpoint, this exception will be hit if a reference to an RDD not defined by the streaming job is used in DStream operations. For more information, See SPARK-13758.\n",
      "  at org.apache.spark.errors.SparkCoreErrors$.rddLacksSparkContextError(SparkCoreErrors.scala:130)\n",
      "  at org.apache.spark.rdd.RDD.sc(RDD.scala:95)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:412)\n",
      "  at countPositive(<console>:25)\n",
      "  at $anonfun$res38$3(<console>:29)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_2$(Unknown Source)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_1$(Unknown Source)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:407)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookSampleRdd.\n",
    "    map(review => (review.id, review.reviewText)).\n",
    "    filter(_._1 != null).\n",
    "    map({case (id, line) => (id, countPositive(line) + countNegative(line))}).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abe811",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sentiment Analysis\n",
    "bookSampleRdd\n",
    "    .join"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
