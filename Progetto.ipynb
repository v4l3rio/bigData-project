{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1708594159698_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-89-154.ec2.internal:20888/proxy/application_1708594159698_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-225SKNZY8MABQ\n",
       "\" application-id=\"application_1708594159698_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-89-20.ec2.internal:8042/node/containerlogs/container_1708594159698_0002_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784984_Valerio_Di_Zio', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1708594159698_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-89-154.ec2.internal:20888/proxy/application_1708594159698_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-225SKNZY8MABQ\n",
       "\" application-id=\"application_1708594159698_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-89-20.ec2.internal:8042/node/containerlogs/container_1708594159698_0002_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73782334d5fd47e1a06491cb7344710c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-vdizio\n",
      "path_book_sample: String = s3a://unibo-bd2324-vdizio/project/book_sample134.json\n",
      "path_book_sample_7gb: String = s3a://unibo-bd2324-vdizio/project/books_sample7gb.json\n",
      "path_positive_words: String = s3a://unibo-bd2324-vdizio/project/positive-words.txt\n",
      "path_negative_words: String = s3a://unibo-bd2324-vdizio/project/negative-words.txt\n",
      "res3: String = SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/application_1708594159698_0002/\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-vdizio\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_book_sample_7gb = \"s3a://\"+bucketname+\"/project/books_sample7gb.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1760096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51be556ce6f443aa0a8c8be8dd90e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.broadcast.Broadcast\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.broadcast.Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caea9ca55d740789d287fa84ff34872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class BookReview\n",
      "defined object BookReview\n",
      "warning: previously defined class BookReview is not a companion to object BookReview.\n",
      "Companions must be defined together; you may wish to use :paste mode for this.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78959e79",
   "metadata": {},
   "source": [
    "## Caricamento del Dataset\n",
    "Il dataset utilizzato è di tipo json, per l'import utilizziamo la funzione ``spark.read`` nello specifico con l'opzione _\"dropmalformed\"_ che ci permette di evitare eventuali problemi nel dataset.\n",
    "Il dataset utilizzato ha una dimensione di ~ 7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08c5c178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b733258bc95a4b23bdd0a3f41a997f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookDataset: org.apache.spark.sql.Dataset[BookReview] = [id: string, overall: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "// È possibile decommentare la linea di codice seguente per utilizzare un dataset di dimensioni ridotte (128MB).\n",
    "//val bookDataset = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample).map(BookReview.extract)\n",
    "\n",
    "val bookDataset = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample_7gb).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2223d61",
   "metadata": {},
   "source": [
    "# Job1 - Sentiment Analysis\n",
    "### Descrizione del Problema\n",
    "L'analisi dei sentimenti è una tecnica utilizzata per determinare il sentimento associato a un certo testo, che può essere positivo, negativo o neutro. \n",
    "Nel contesto delle recensioni di prodotti su Amazon, l'analisi dei sentimenti può essere di grande valore per comprendere il feedback dei clienti e identificare le opinioni predominanti su un determinato prodotto.\n",
    "\n",
    "Questo notebook affronta il problema dell'analisi dei sentimenti sulle recensioni di libri venduti su Amazon utilizzando l'approccio delle Word List:\n",
    "\n",
    "Liste di Parole Positive e Negative: Questo metodo si basa sull'utilizzo di liste di parole positive e negative predefinite. Ogni parola nella recensione viene confrontata con le parole presenti nelle liste positive e negative per determinare il sentiment complessivo della recensione.\n",
    "\n",
    "Nel seguente notebook, esploreremo come implementare l'analisi dei sentimenti utilizzando liste di parole positive e negative, fornendo un esempio pratico di come applicare questo approccio per valutare il sentiment di una recensione di libri su Amazon. Successivamente verrà calcolato l'errore confrontando i voti predetti con i voti effettivi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f6665",
   "metadata": {},
   "source": [
    "### Creazione delle liste di parole positive e negaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b272049e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5366bf21ed40a7b76f70993b08fa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positiveWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/positive-words.txt MapPartitionsRDD[267] at textFile at <console>:34\n",
      "negativeWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/negative-words.txt MapPartitionsRDD[269] at textFile at <console>:34\n"
     ]
    }
   ],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec55c5",
   "metadata": {},
   "source": [
    "### Split and Clean\n",
    "Definiamo la funzione ``splitAndClean()`` che accetta come parametro l'id della recensione con il suo testo.\n",
    "Restituendo un array dove l'id verrà associato ad ogni parola della recensione (split) a cui verrà dato come peso il valore 1, che permetterà di aumentare o diminuire il punteggio assegnato alla frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eea7ed53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bfcda447744dc58adb6a9f8979865b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitAndClean: (id: String, text: String)Array[(String, String, Int)]\n"
     ]
    }
   ],
   "source": [
    "def splitAndClean(id:String, text: String): Array[(String, String, Int)] = \n",
    "    text.split(\"\\\\s+\").map(el => (id, el.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \"\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6868",
   "metadata": {},
   "source": [
    "## Ottimizzazione Job 1\n",
    "Il processo di ottimizzazione non è stato immediato, inizialmente abbiamo pensato di implementare una versione semplice dell'applicativo, utilizzando un dataset di dimensioni ridotte su cui poter testare la logica.\n",
    "Successivamente sono state applicate 2 ottimizzazioni principali:\n",
    "\n",
    "1. Utilizzo di variabili broadcast per le liste di parole uguali per tutti\n",
    "2. Utilizzo di meccanismi di cache\n",
    "3. Ottimizzazione delle strutture dati per evitare conversioni inutili di tipi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37560965",
   "metadata": {},
   "source": [
    "## Job 1 - Prima fase, conta delle parole\n",
    "\n",
    "All'interno del blocco di codice che segue vi è il core dell'elaborazione.\n",
    "\n",
    "Sono state definite delle funzioni che si occupano di contare il numero di occorrenze di una parola in una determinata frase e di ottenere un risultato nella forma: ```(\"id frase\", \"numero parole positive\" o \"numero parole negative\")```.\n",
    "Al risultato ottenuto viene applicato un join con opportuna sottrazione (positive - negative) ottenendo così un \"peso\" che verrà utilizzato successivamente per capire se una frase esprime un sentimento positivo o negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c408134625b9412eb923a058c93438a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countOccurrenciesOfWordReview: (reviews: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
      "joinAndMapWords: (review: org.apache.spark.sql.DataFrame, wordsDataset: org.apache.spark.broadcast.Broadcast[Array[String]])org.apache.spark.sql.DataFrame\n",
      "reviewsDataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, word: string ... 1 more field]\n",
      "positiveWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(89)\n",
      "negativeWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(91)\n",
      "withPositiveDF: org.apache.spark.sql.DataFrame = [id: string, occurrencies: bigint]\n",
      "withNegativeDF: org.apache.spark.sql.DataFrame = [id: string, occurrencies: bigint]\n",
      "reducedResult: org.apache.spark.sql.DataFrame = [id: string, valuePositive: bigint ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def countOccurrenciesOfWordReview(reviews: DataFrame): DataFrame = \n",
    "    reviews.\n",
    "        groupBy(\"id\", \"word\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "\n",
    "\n",
    "def joinAndMapWords(review:DataFrame, wordsDataset: Broadcast[Array[String]]): DataFrame = {\n",
    "    val wordDF = review.sparkSession.createDataFrame(wordsDataset.value.map(Tuple1.apply)).toDF(\"word\")\n",
    "    \n",
    "    val aggregRevDF = countOccurrenciesOfWordReview(review)\n",
    "    \n",
    "    return aggregRevDF.\n",
    "        join(wordDF, aggregRevDF(\"word\") === wordDF(\"word\")).\n",
    "        groupBy(\"id\").\n",
    "        agg(sum(\"occurrencies\").alias(\"occurrencies\"))\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "val reviewsDataset = bookDataset.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    filter(review => review.getAs[String](\"reviewText\") != null && review.getAs[String](\"reviewText\").length > 3).\n",
    "    flatMap(review => splitAndClean(review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    filter(row => row._2.length > 3).\n",
    "    toDF(\"id\", \"word\", \"occurrencies\").\n",
    "    cache()\n",
    "\n",
    "val positiveWordsBroadcast = sc.broadcast(positiveWords.collect())\n",
    "val negativeWordsBroadcast = sc.broadcast(negativeWords.collect())\n",
    "\n",
    "\n",
    "val withPositiveDF = joinAndMapWords(reviewsDataset, positiveWordsBroadcast)\n",
    "    \n",
    "val withNegativeDF = joinAndMapWords(reviewsDataset, negativeWordsBroadcast)\n",
    "\n",
    "\n",
    "val reducedResult = withPositiveDF.\n",
    "    join(withNegativeDF, Seq(\"id\"), \"outer\").\n",
    "    toDF(\"id\", \"valuePositive\", \"valueNegative\").\n",
    "    withColumn(\"weight\", coalesce(col(\"valuePositive\"), lit(0L)) - coalesce(col(\"valueNegative\"), lit(0L)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fb8aa",
   "metadata": {},
   "source": [
    "# Job 1 - Seconda fase, calcolo dei risultati \n",
    "\n",
    "Successivamente calcoliamo il risultato ottenuto utilizzando una normalizzazione Z-Score, successivamente normalizzata a sua volta tramite Min-Max tra 1 e 5 così da ottenere un voto confrontabile con il metodo utilizzato da Amazon, le stelle che possono essere 1,2,3,4 o 5.\n",
    "Il valore così ottenuto viene poi confrontato con quello presente nel dataset (overall), per verificare la correttezza delle nostre previsioni calcolando un errore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d78cec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a12d03f406c401da79123d0bf51a3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN_TRESHOLD: Int = -10\n",
      "MAX_TRESHOLD: Int = 10\n",
      "resultDataframeRaw: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, weight: bigint]\n",
      "resultDataframe: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, weight: bigint]\n",
      "meanAndStdDF: Array[org.apache.spark.sql.Row] = Array([1.7176823885475228,2.577777605910873])\n",
      "meanValue: Double = 1.7176823885475228\n",
      "stdValue: Double = 2.577777605910873\n",
      "normalizedDF: org.apache.spark.sql.DataFrame = [id: string, weight: bigint ... 1 more field]\n",
      "minMaxDF: Array[org.apache.spark.sql.Row] = Array([-4.1577218934526226,2.8250371927950813])\n",
      "minZScore: Double = -4.1577218934526226\n",
      "maxZScore: Double = 2.8250371927950813\n",
      "normalizedAndScaledDF: org.apache.spark.sql.DataFrame = [id: string, estimated_overall: double]\n"
     ]
    }
   ],
   "source": [
    "val MIN_TRESHOLD = -10\n",
    "val MAX_TRESHOLD = 10\n",
    "\n",
    "val resultDataframeRaw = reducedResult.select(\"id\", \"weight\").cache()\n",
    "val resultDataframe = resultDataframeRaw.filter( ($\"weight\" < MAX_TRESHOLD) && ($\"weight\" > MIN_TRESHOLD))\n",
    "\n",
    "// Calculate mean and standard deviation\n",
    "\n",
    "val meanAndStdDF = resultDataframe.agg(mean(\"weight\"), stddev(\"weight\")).collect()\n",
    "val meanValue = meanAndStdDF(0)(0).asInstanceOf[Double]\n",
    "val stdValue = meanAndStdDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "// Normalize the data using Z-score normalization formula\n",
    "\n",
    "val normalizedDF = resultDataframe.withColumn(\"z_score\", (col(\"weight\") - meanValue) / stdValue)\n",
    "\n",
    "// Scale the normalized values to be between -1 and 1\n",
    "val minMaxDF = normalizedDF.agg(min(\"z_score\"), max(\"z_score\")).collect()\n",
    "val minZScore = minMaxDF(0)(0).asInstanceOf[Double]\n",
    "val maxZScore = minMaxDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "\n",
    "val normalizedAndScaledDF = normalizedDF.withColumn(\"estimated_overall\", \n",
    "                                                    (col(\"z_score\") - minZScore) / (maxZScore - minZScore) * 4 + 1).\n",
    "    select(\"id\", \"estimated_overall\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4fa836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b3301808984554b37fabd22374bcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuale di elementi scartati: 4.315023709934394 %\n"
     ]
    }
   ],
   "source": [
    "println(\"Percentuale di elementi scartati: \"+(resultDataframeRaw.count().toDouble - resultDataframe.count()) / resultDataframe.count() * 100 + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42b92ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891a19692dc4047b9a8c1d45630abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relativeErrors: org.apache.spark.sql.DataFrame = [id: string, real_value: double ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val relativeErrors = \n",
    "bookDataset.\n",
    "    map(review => (review.id, review.overall)).\n",
    "    toDF(\"id\", \"real_value\").\n",
    "    join(normalizedAndScaledDF, Seq(\"id\")).\n",
    "    withColumn(\"relative_error\", abs(coalesce(col(\"real_value\"), lit(3.0)) - \n",
    "                                     coalesce(col(\"estimated_overall\"), lit(3.0))) / \n",
    "                                   coalesce(col(\"real_value\"), lit(3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2bec6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ec745afe949bfa50d4b9d5615bca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumOfRelativeErrors: Double = 3182696.14258182\n",
      "meanError: Double = 0.3490734623103762\n",
      "Errore Medio: 34.90734623103762%\n"
     ]
    }
   ],
   "source": [
    "val sumOfRelativeErrors = relativeErrors.select(\"relative_error\").agg(sum(\"relative_error\")).\n",
    "                                collect()(0).getDouble(0)\n",
    "val meanError =  sumOfRelativeErrors / relativeErrors.count\n",
    "\n",
    "println(\"Errore Medio: \" + meanError*100 + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea502d1",
   "metadata": {},
   "source": [
    "# Job 2 - Frequent Pattern Mining\n",
    "\n",
    "Qui inizia il secondo Job, dove effettuiamo un Frequent Pattern Mining utilizzando le funzionalità di Spark.\n",
    "\n",
    "## Creazione del DataFrame delle reviews\n",
    "Prima di tutto andiamo a definire una funzione di utility che ci permette di eliminare eventuali caratteri estranei dai testi delle recensioni (es. virgole, punti etc.). \n",
    "\n",
    "Dopodiché, utilizziamo il Dataset caricato anche per il primo Job e ne prendiamo solo le colonne `\"id\"` e `\"reviewText\"`, sono le uniche che ci servono. In modo simile al primo Job, eliminiamo le recensioni duplicate usando ```dropDuplicates```: questo è necessario perché esplorando il Dataset sono state trovate recensioni identiche per lo stesso prodotto, facenti riferimento a diverse versioni dello stesso.\n",
    "\n",
    "Per comodità, il Dataset viene convertito in un RDD in modo da svolgere le successive operazioni di **filter** e **map**. Infine, otteniamo il risultato in forma di DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "682a8692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e717c13d79674a3f8f2b4d55e57a4670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean: (text: String)String\n"
     ]
    }
   ],
   "source": [
    "def clean(text: String): String =\n",
    "    text.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09ad9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd92025d458844caa8da176ccf7d097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, text: string]\n"
     ]
    }
   ],
   "source": [
    "val reviewsDF = bookDataset.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    map(review => (review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    rdd.\n",
    "    filter({case (_, text) => text != null && text.length > 3}).\n",
    "    map({case (id, text) => (id, clean(text))}).\n",
    "    toDF(\"id\", \"text\").\n",
    "    cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b674de6",
   "metadata": {},
   "source": [
    "## Creazione di Tokenizer e StopWord remover\n",
    "\n",
    "Queste due funzioni sono utili per **dividere i testi in parole** e **rimuovere le stop words** (parole comunemente utilizzate in un linguaggio ma normalmente prive di valore se prese da sole, per esempio \"is\", \"the\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b73a7823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a5e6b0875d47438899a9a650a18564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
      "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_77013fd3722c\n",
      "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_47374cf8348d, numStopWords=181, locale=en_US, caseSensitive=false\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "\n",
    "val tokenizer = new Tokenizer().\n",
    "    setInputCol(\"text\").\n",
    "    setOutputCol(\"words\")\n",
    "\n",
    "val remover = new StopWordsRemover().\n",
    "    setInputCol(\"words\").\n",
    "    setOutputCol(\"filtered_words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a04f3",
   "metadata": {},
   "source": [
    "## Pulizia del Dataset\n",
    "\n",
    "Nella cella seguente andiamo a utilizzare il `tokenizer` e il `remover` creati precedentemente per pulire ulteriormente il DataFrame. \n",
    "\n",
    "Osservando i risultati di `filteredWordsDF` si può notare che sono ancora presenti refusi e stringhe contenenti spazi. Oltre a questo, la classe che verrà utilizzata successivamente, ovvero `FPGrowth`, richiede che all'interno di ogni pattern siano presenti stringhe **univoche**, motivo per cui andiamo a svolgere l'operazione `toArray.distinct`, che rimuove elementi duplici all'interno di un'Array di Scala)\n",
    "\n",
    "Otteniamo, infine, un DataFrame da una sola colonna, contenente un Array di parole per ciascuna recensione. È possibile notare che non sono più presenti gli ID: questi infatti non sono più necessari al fine di computare i Frequent Pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c5cc010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea03ac3c13f447a84b6588102d5e15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.collection.mutable.WrappedArray\n",
      "wordsDF: org.apache.spark.sql.DataFrame = [id: string, text: string ... 1 more field]\n",
      "filteredWordsDF: org.apache.spark.sql.DataFrame = [id: string, text: string ... 2 more fields]\n",
      "uniqueWordsDF: org.apache.spark.sql.DataFrame = [words: array<string>]\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    "\n",
    "val wordsDF = tokenizer.transform(reviewsDF)\n",
    "val filteredWordsDF = remover.transform(wordsDF)\n",
    "\n",
    "val uniqueWordsDF = filteredWordsDF.\n",
    "    select(\"filtered_words\").\n",
    "    distinct().\n",
    "    map(item => item.getAs[WrappedArray[String]](0).filter(_.length > 1).toArray.distinct).\n",
    "    toDF(\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956c265",
   "metadata": {},
   "source": [
    "## Utilizzo di FPGrowth\n",
    "\n",
    "Nella cella seguente definiamo una funzione che ci permette di utilizzare facilmente la classe di `FPGrowth` per calcolare i Frequent Pattern di un DataFrame. Questa istanzia la classe in questione, fissando gli iper-parametri a dei valori arbitrari: in particolare, il **supporto minimo** per considerare un pattern *frequent* e la **confidenza minima**, ovvero (dalla documentazione di Spark) un'indicazione di quanto spesso una *association rule* viene trovata vera. Quest'ultima non influenza la ricerca dei Frequent Pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1553cd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356182559b484dc695d927ee514aaef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.fpm.FPGrowth\n",
      "frequentItemsets: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "\n",
    "def frequentItemsets(df: DataFrame): DataFrame = {\n",
    "    val fpg = new FPGrowth().\n",
    "        setItemsCol(\"words\").\n",
    "        setMinSupport(0.5).\n",
    "        setMinConfidence(0.5)\n",
    "\n",
    "    val model = fpg.fit(df)\n",
    "    return model.freqItemsets\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44fbd",
   "metadata": {},
   "source": [
    "## Visualizzazione dei risultati\n",
    "\n",
    "Se effettuiamo una prima visualizzazione dei risultati è possibile notare fin da subito come i risultati più frequenti siano parole come \"libro\" e \"leggere\", risultati piuttosto predicibili. \n",
    "Andiamo quindi a creare un nuovo dataset applicando una blacklist, ovvero parole da escludere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dfd38",
   "metadata": {},
   "source": [
    "## Secondo Dataset, rimuovendo parole triviali (es. \"book\", \"read\" etc.)\n",
    "\n",
    "Per eliminare queste parole dalla ricerca, effettuiamo un'ultima modifica al nostro DataFrame, eliminando parole triviali come \"libro\" e \"leggere\", prese da un file opportunamente creato per lo scopo. Questo viene poi tramutato in un array contenuto in una **broadcast variable**. Infine, ogni array di parole viene modificato in modo da sottrarre queste parole alla ricerca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b51f6a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8c6215075644f4a2b97bb63079ed5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_blacklist_words: String = s3a://unibo-bd2324-vdizio/project/blacklist.txt\n",
      "blacklistWords: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(5)\n",
      "prunedDF: org.apache.spark.sql.DataFrame = [words: array<string>]\n"
     ]
    }
   ],
   "source": [
    "// caricamento della blacklist delle parole da escludere\n",
    "val path_blacklist_words = \"s3a://\"+bucketname+\"/project/blacklist.txt\"\n",
    "\n",
    "val blacklistWords = sc.broadcast(sc.textFile(path_blacklist_words).collect)\n",
    "\n",
    "val prunedDF = uniqueWordsDF.\n",
    "    map(item => item.getAs[WrappedArray[String]](0).toArray.filterNot(blacklistWords.value.contains)).\n",
    "    toDF(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997ce7e",
   "metadata": {},
   "source": [
    "## Utilizzo di FPGrowth sul DataFrame ottenuto\n",
    "\n",
    "Il risultato finale appare più interessante. Questo potrebbe essere ulteriormente migliorato escludendo più parole dalla ricerca.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d666b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7069c4f8c7c4d5cb0b9d8ffcda0f375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c828cb020474529abe0571ff1b21f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequentItemsets(prunedDF).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9354e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
