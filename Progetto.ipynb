{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa7170",
   "metadata": {},
   "source": [
    "# Progetto BigData\n",
    "## Valerio Di Zio e Francesco Magnani\n",
    "\n",
    "intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a280583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorMemory': '8G', 'numExecutors': 2, 'executorCores': 3, 'conf': {'spark.dynamicAllocation.enabled': 'false'}, 'proxyUser': 'assumed-role_voclabs_user2784984_Valerio_Di_Zio', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9695ac4528f746f09612cb3b26f08e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1707745135143_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-80-213.ec2.internal:20888/proxy/application_1707745135143_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-3RBULBBV0K8AM\n",
       "\" application-id=\"application_1707745135143_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-88-212.ec2.internal:8042/node/containerlogs/container_1707745135143_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2324-vdizio\n",
      "path_book_sample: String = s3a://unibo-bd2324-vdizio/project/book_sample134.json\n",
      "path_book_sample32row: String = s3a://unibo-bd2324-vdizio/project/book_sample32row.json\n",
      "path_positive_words: String = s3a://unibo-bd2324-vdizio/project/positive-words.txt\n",
      "path_negative_words: String = s3a://unibo-bd2324-vdizio/project/negative-words.txt\n",
      "res3: String = SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/application_1707745135143_0001/\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2324-vdizio\"\n",
    "\n",
    "val path_book_sample = \"s3a://\"+bucketname+\"/project/book_sample134.json\"\n",
    "val path_book_sample32row = \"s3a://\"+bucketname+\"/project/book_sample32row.json\"\n",
    "val path_positive_words = \"s3a://\"+bucketname+\"/project/positive-words.txt\"\n",
    "val path_negative_words = \"s3a://\"+bucketname+\"/project/negative-words.txt\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1760096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164f234524b343969459c679232f0f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.broadcast.Broadcast\n",
      "import org.apache.spark.HashPartitioner\n",
      "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@28\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val partitioner = new HashPartitioner(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f4d6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d8289ecc3f4bb58ea48972af49e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class BookReview\n",
      "defined object BookReview\n",
      "warning: previously defined class BookReview is not a companion to object BookReview.\n",
      "Companions must be defined together; you may wish to use :paste mode for this.\n"
     ]
    }
   ],
   "source": [
    "case class BookReview(\n",
    "    id: String,\n",
    "    overall:Double,\n",
    "    reviewTime:String,\n",
    "    reviewerID:String,\n",
    "    asin:String,\n",
    "    reviewText:String,\n",
    "    summary:String\n",
    ")\n",
    "\n",
    "object BookReview{\n",
    "    def extract(row:org.apache.spark.sql.Row) = {\n",
    "        val overall = row.getAs[Double](\"overall\")\n",
    "        val reviewTime = row.getAs[String](\"reviewTime\")\n",
    "        val reviewerID = row.getAs[String](\"reviewerID\")\n",
    "        val asin = row.getAs[String](\"asin\")\n",
    "        val reviewText = row.getAs[String](\"reviewText\")\n",
    "        val summary = row.getAs[String](\"summary\")\n",
    "        val id = reviewerID + \"-\" + asin\n",
    "    \n",
    "        new BookReview(id, overall, reviewTime, reviewerID, asin, reviewText, summary)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "52289f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9880753950ba44d29f2c4064006f80c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookSampleRdd32row: org.apache.spark.sql.Dataset[BookReview] = [id: string, overall: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "val bookSampleRdd32row = spark.read.option(\"mode\", \"DROPMALFORMED\").json(path_book_sample).map(BookReview.extract)\n",
    "//val bookSampleRdd32row = spark.read.json(path_book_sample32row).map(BookReview.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b272049e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b86e46566a44f0b21ccf2644f720c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positiveWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/positive-words.txt MapPartitionsRDD[1928] at textFile at <console>:42\n",
      "negativeWords: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2324-vdizio/project/negative-words.txt MapPartitionsRDD[1930] at textFile at <console>:42\n"
     ]
    }
   ],
   "source": [
    "val positiveWords = sc.textFile(path_positive_words)\n",
    "val negativeWords = sc.textFile(path_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "eea7ed53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf39b0134cc4aa793b07d158c92b896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitAndClean: (id: String, text: String)Array[(String, String, Int)]\n"
     ]
    }
   ],
   "source": [
    "def splitAndClean(id:String, text: String): Array[(String, String, Int)] = \n",
    "    text.split(\"\\\\s+\").map(el => (id, el.toLowerCase.replaceAll(\"[^a-zA-Z\\\\s]\", \"\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6868",
   "metadata": {},
   "source": [
    "# Tentativi di ottimizzazione\n",
    "1. utilizzo di variabili broadcast per le liste di parole uguali per tutti\n",
    "2. ripartizione dei dataset per ottimizzare il join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b6d39569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5d494f3bfc4e169c362350facf2e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joinAndMapWords: (review: org.apache.spark.sql.DataFrame, wordsDataset: org.apache.spark.broadcast.Broadcast[Array[String]])org.apache.spark.rdd.RDD[(String, Int)]\n",
      "reviewsRdd: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, word: string ... 1 more field]\n",
      "positiveWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(620)\n",
      "negativeWordsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(622)\n",
      "withPositiveRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[1963] at reduceByKey at <console>:55\n",
      "withNegativeRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[1982] at reduceByKey at <console>:55\n",
      "reducedResult: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1986] at map at <console>:46\n"
     ]
    }
   ],
   "source": [
    "def joinAndMapWords(review:DataFrame, wordsDataset: Broadcast[Array[String]]): RDD[(String, Int)] = {\n",
    "    val df = review.sparkSession.createDataFrame(wordsDataset.value.map(Tuple1.apply)).toDF(\"word\")\n",
    "    val aggregRev = review.\n",
    "        map (row => ((row.getAs[String](\"id\"), row.getAs[String](\"word\")), row.getAs[Int](\"occurrencies\"))).\n",
    "        rdd.\n",
    "        reduceByKey(_ + _).\n",
    "        map{ case ((k1, k2), v1) => (k1, k2, v1)}\n",
    "    \n",
    "        val aggregRevDF = aggregRev.toDF(\"id\", \"word\", \"occurrencies\")\n",
    "    \n",
    "        return aggregRevDF.\n",
    "            join(df, aggregRevDF(\"word\") === df(\"word\")).\n",
    "            map(row => (row.getAs[String](\"id\"), row.getAs[Int](\"occurrencies\"))).\n",
    "            rdd.\n",
    "            reduceByKey(_ + _)\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "val reviewsRdd = bookSampleRdd32row.\n",
    "    select(\"id\", \"reviewText\").\n",
    "    dropDuplicates(Seq(\"id\")).\n",
    "    filter(review => review.getAs[String](\"reviewText\") != null && review.getAs[String](\"reviewText\").length > 3).\n",
    "    flatMap(review => splitAndClean(review.getAs[String](\"id\"), review.getAs[String](\"reviewText\"))).\n",
    "    filter(row => row._2.length > 3).\n",
    "    toDF(\"id\", \"word\", \"occurrencies\").\n",
    "    cache()\n",
    "\n",
    "val positiveWordsBroadcast = sc.broadcast(positiveWords.collect())\n",
    "val negativeWordsBroadcast = sc.broadcast(negativeWords.collect())\n",
    "\n",
    "\n",
    "val withPositiveRdd = joinAndMapWords(reviewsRdd, positiveWordsBroadcast)\n",
    "    \n",
    "val withNegativeRdd = joinAndMapWords(reviewsRdd, negativeWordsBroadcast)\n",
    "\n",
    "\n",
    "val reducedResult = withPositiveRdd.\n",
    "    fullOuterJoin(withNegativeRdd).\n",
    "     map ({ case (key, (value1, value2)) => (key, (value1.getOrElse(0) - value2.getOrElse(0)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5d78cec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afb40606bde4b1cb28f7ba3afbd61fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultDataframe: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, value: int]\n",
      "minValue: Double = -82.0\n",
      "maxValue: Double = 91.0\n",
      "meanAndStdDF: Array[org.apache.spark.sql.Row] = Array([1.8820141631722722,3.570195472560125])\n",
      "meanValue: Double = 1.8820141631722722\n",
      "stdValue: Double = 3.570195472560125\n",
      "normalizedDF: org.apache.spark.sql.DataFrame = [key: string, value: int ... 1 more field]\n",
      "minMaxDF: Array[org.apache.spark.sql.Row] = Array([-23.495076056163935,24.96165448692443])\n",
      "minZScore: Double = -23.495076056163935\n",
      "maxZScore: Double = 24.96165448692443\n",
      "normalizedAndScaledDF: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[2742] at map at <console>:46\n",
      "mapIntToOverallVote: (toMap: Double, fromMin: Double, fromMax: Double, toMin: Double, toMax: Double)Double\n"
     ]
    }
   ],
   "source": [
    "val resultDataframe = reducedResult.toDF(\"key\", \"value\").cache()\n",
    "\n",
    "val minValue = resultDataframe.agg(min(\"value\")).head().getInt(0).toDouble\n",
    "val maxValue = resultDataframe.agg(max(\"value\")).head().getInt(0).toDouble\n",
    "\n",
    "\n",
    "// Calculate mean and standard deviation\n",
    "val meanAndStdDF = resultDataframe.agg(mean(\"value\"), stddev(\"value\")).collect()\n",
    "val meanValue = meanAndStdDF(0)(0).asInstanceOf[Double]\n",
    "val stdValue = meanAndStdDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "// Normalize the data using Z-score normalization formula\n",
    "// Normalize the data using Z-score normalization formula\n",
    "val normalizedDF = resultDataframe.withColumn(\"z_score\", (col(\"value\") - meanValue) / stdValue)\n",
    "\n",
    "// Scale the normalized values to be between -1 and 1\n",
    "val minMaxDF = normalizedDF.agg(min(\"z_score\"), max(\"z_score\")).collect()\n",
    "val minZScore = minMaxDF(0)(0).asInstanceOf[Double]\n",
    "val maxZScore = minMaxDF(0)(1).asInstanceOf[Double]\n",
    "\n",
    "val normalizedAndScaledDF = normalizedDF.withColumn(\"normalized_value\", \n",
    "  (col(\"z_score\") - minZScore) / (maxZScore - minZScore) * 4 + 1\n",
    ").select(\"key\", \"normalized_value\").rdd.map(row => (row.getAs[String](0), row.getAs[Double](1)))\n",
    "\n",
    "\n",
    "\n",
    "// a = min, b = max, c = 1 e d = 5\n",
    "def mapIntToOverallVote(toMap: Double, fromMin: Double, fromMax: Double, toMin:Double=1, toMax:Double=5): Double =\n",
    "     toMin + ((toMax-toMin)/(fromMax-fromMin))*(toMap-fromMin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "78396ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099d87e6211b474dae5f658c8c131f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: double (nullable = false)\n",
      "\n",
      "normalizedResult: Unit = ()\n"
     ]
    }
   ],
   "source": [
    "val normalizedResult = \n",
    "resultDataframe.\n",
    "    map({row => (row.getAs[String](0), mapIntToOverallVote(row.getAs[Int](1).toDouble, minValue, maxValue))}).printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "42b92ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e73c9f86643d482ea07016411212b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relativeErrors: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[2753] at map at <console>:46\n"
     ]
    }
   ],
   "source": [
    "val relativeErrors = \n",
    "bookSampleRdd32row.\n",
    "    map(review => (review.id, review.overall)).rdd.\n",
    "    fullOuterJoin(normalizedAndScaledDF).\n",
    "    map ({ case (key, (real, estimated)) => \n",
    "            (key, Math.abs(real.getOrElse[Double](3) - estimated.getOrElse[Double](3)) / real.getOrElse[Double](3) )\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c2bec6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e6517836b1469a955c84b1d8c2a9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanError: Double = 0.42554253489305555\n",
      "Errore Medio: 42.55425348930555%\n"
     ]
    }
   ],
   "source": [
    "val meanError = (relativeErrors.map(row => row._2).sum / relativeErrors.count)\n",
    "\n",
    "println(\"Errore Medio: \" + meanError*100 + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b1fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a7823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
